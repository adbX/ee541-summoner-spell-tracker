@misc{belvalTextRecognitionDataGenerator2023,
  title = {{{TextRecognitionDataGenerator}}},
  author = {Belval, Edouard},
  year = {2023},
  month = apr,
  urldate = {2023-04-15},
  abstract = {A synthetic data generator for text recognition},
  copyright = {MIT},
  keywords = {data,dataset,fake,ocr,synthetic,text,text-recognition,training-set-generator}
}

@inproceedings{etterSyntheticRecipeOCR2019,
  title = {A {{Synthetic Recipe}} for {{OCR}}},
  booktitle = {2019 {{International Conference}} on {{Document Analysis}} and {{Recognition}} ({{ICDAR}})},
  author = {Etter, David and Rawls, Stephen and Carpenter, Cameron and Sell, Gregory},
  year = {2019},
  month = sep,
  pages = {864--869},
  publisher = {{IEEE}},
  address = {{Sydney, Australia}},
  doi = {10.1109/ICDAR.2019.00143},
  urldate = {2023-04-15},
  abstract = {Synthetic data generation for optical character recognition (OCR) promises unlimited training data at zero annotation cost. With enough fonts and seed text, we should be able to generate data to train a model that approaches or exceeds the performance with real annotated data. Unfortunately, this is not always the reality. Unconstrained image settings, such as internet memes, scanned web pages, or newspapers, present diverse scripts, fonts, layouts, and complex backgrounds, which cause models trained with synthetic data to break down. In this work, we investigate the synthetic image generation problem on a large multilingual set of unconstrained document images. Our work presents a comprehensive evaluation of the impact of synthetic data attributes on model performance. The results provide a recipe for synthetic data generation that will help guide future research.},
  isbn = {978-1-72813-014-9},
  langid = {english},
  file = {C\:\\Users\\adb\\Zotero\\storage\\MA7D3SFL\\Etter et al. - 2019 - A Synthetic Recipe for OCR.pdf}
}

@misc{heDeepResidualLearning2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = dec,
  number = {arXiv:1512.03385},
  eprint = {arXiv:1512.03385},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1512.03385},
  urldate = {2023-04-15},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\adb\\Zotero\\storage\\V8S56FUP\\He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf;C\:\\Users\\adb\\Zotero\\storage\\GVKZ6D9S\\1512.html}
}

@misc{jaderbergSyntheticDataArtificial2014,
  title = {Synthetic {{Data}} and {{Artificial Neural Networks}} for {{Natural Scene Text Recognition}}},
  author = {Jaderberg, Max and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  year = {2014},
  month = dec,
  number = {arXiv:1406.2227},
  eprint = {arXiv:1406.2227},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1406.2227},
  urldate = {2023-04-15},
  abstract = {In this work we present a framework for the recognition of natural scene text. Our framework does not require any human-labelled data, and performs word recognition on the whole image holistically, departing from the character based recognition systems of the past. The deep neural network models at the centre of this framework are trained solely on data produced by a synthetic text generation engine -- synthetic data that is highly realistic and sufficient to replace real data, giving us infinite amounts of training data. This excess of data exposes new possibilities for word recognition models, and here we consider three models, each one "reading" words in a different way: via 90k-way dictionary encoding, character sequence encoding, and bag-of-N-grams encoding. In the scenarios of language based and completely unconstrained text recognition we greatly improve upon state-of-the-art performance on standard datasets, using our fast, simple machinery and requiring zero data-acquisition costs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\adb\\Zotero\\storage\\6PKNBWXK\\Jaderberg et al. - 2014 - Synthetic Data and Artificial Neural Networks for .pdf;C\:\\Users\\adb\\Zotero\\storage\\L5BSLKD7\\1406.html}
}

@misc{simonyanVeryDeepConvolutional2015,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  year = {2015},
  month = apr,
  number = {arXiv:1409.1556},
  eprint = {arXiv:1409.1556},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1409.1556},
  urldate = {2023-04-15},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\adb\\Zotero\\storage\\S6MQ7PMR\\Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf;C\:\\Users\\adb\\Zotero\\storage\\2JQNXLGJ\\1409.html}
}

@misc{yimSynthTIGERSyntheticText2021,
  title = {{{SynthTIGER}}: {{Synthetic Text Image GEneratoR Towards Better Text Recognition Models}}},
  shorttitle = {{{SynthTIGER}}},
  author = {Yim, Moonbin and Kim, Yoonsik and Cho, Han-Cheol and Park, Sungrae},
  year = {2021},
  month = jul,
  number = {arXiv:2107.09313},
  eprint = {arXiv:2107.09313},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2107.09313},
  urldate = {2023-04-15},
  abstract = {For successful scene text recognition (STR) models, synthetic text image generators have alleviated the lack of annotated text images from the real world. Specifically, they generate multiple text images with diverse backgrounds, font styles, and text shapes and enable STR models to learn visual patterns that might not be accessible from manually annotated data. In this paper, we introduce a new synthetic text image generator, SynthTIGER, by analyzing techniques used for text image synthesis and integrating effective ones under a single algorithm. Moreover, we propose two techniques that alleviate the long-tail problem in length and character distributions of training data. In our experiments, SynthTIGER achieves better STR performance than the combination of synthetic datasets, MJSynth (MJ) and SynthText (ST). Our ablation study demonstrates the benefits of using sub-components of SynthTIGER and the guideline on generating synthetic text images for STR models. Our implementation is publicly available at https://github.com/clovaai/synthtiger.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\adb\\Zotero\\storage\\P7MYJE2M\\Yim et al. - 2021 - SynthTIGER Synthetic Text Image GEneratoR Towards.pdf;C\:\\Users\\adb\\Zotero\\storage\\Z7NQ3IR2\\2107.html}
}

@misc{huangDenselyConnectedConvolutional2018,
  title = {Densely {{Connected Convolutional Networks}}},
  author = {Huang, Gao and Liu, Zhuang and {van der Maaten}, Laurens and Weinberger, Kilian Q.},
  year = {2018},
  month = jan,
  number = {arXiv:1608.06993},
  eprint = {arXiv:1608.06993},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1608.06993},
  urldate = {2023-04-15},
  abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\adb\\Zotero\\storage\\CB3L5VU4\\Huang et al. - 2018 - Densely Connected Convolutional Networks.pdf}
}
